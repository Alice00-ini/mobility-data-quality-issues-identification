{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62260dc5-e666-45dd-af79-3bf513391e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/work/lyub2/.conda_envs/lb-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import os\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0014b20-4a63-4eda-b8ef-8b439a6a4fc2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 集大成！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6330ac1e-7ee2-4fda-a4b6-ea189bc1a01d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8f671531a44410b13b6664ec5beede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing folders:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4571111\n"
     ]
    }
   ],
   "source": [
    "parent_directory = '/scratch/work/lyub2/Problem_output_September/'\n",
    "\n",
    "df_list = []\n",
    "total_len = 0\n",
    "\n",
    "# Iterate through all 30 folders with tqdm for progress bar\n",
    "for day in tqdm(range(1, 31), desc=\"Processing folders\"):\n",
    "    # Generate folder name in the format 'MMDD_output_csv' (e.g., '0901_output_csv')\n",
    "    folder_name = f\"09{str(day).zfill(2)}_output_csv\"\n",
    "    # Define the full path to the csv file\n",
    "    file_path = os.path.join(parent_directory, folder_name, 'P3_unfiltered_tripchain_travelling_time.csv')\n",
    "    \n",
    "    # Check if the file exists to avoid errors\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        continue\n",
    "    \n",
    "    # Read the current CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    total_len += len(df)\n",
    "    df_list.append(df)\n",
    "    \n",
    "\n",
    "df_p3 = pd.concat(df_list, ignore_index=True)\n",
    "print(total_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4e7b47a-9f58-428f-adb4-9c040343bded",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p3.to_csv('/scratch/work/lyub2/Problem_output_September/P3/df_p3.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2df7a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p3 = pd.read_csv(\"/scratch/work/lyub2/Problem_output_September/P3/df_p3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "208d4ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp_device</th>\n",
       "      <th>tripchain_id</th>\n",
       "      <th>total_duration_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>004BbUb8ewhh</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>005eJfiOF2pc</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>005eJfiOF2pc</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>005eJfiOF2pc</td>\n",
       "      <td>2.0</td>\n",
       "      <td>44.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005kboCECfb0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    temp_device  tripchain_id  total_duration_minutes\n",
       "0  004BbUb8ewhh           0.0                    15.0\n",
       "1  005eJfiOF2pc           0.0                    45.0\n",
       "2  005eJfiOF2pc           1.0                    15.0\n",
       "3  005eJfiOF2pc           2.0                    44.7\n",
       "4  005kboCECfb0           0.0                     3.2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_p3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1071852",
   "metadata": {},
   "source": [
    "## 在P1的基础上去除P2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71df4fd9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 先把需要的df1搞出来 30天的太大了 不能一起出来，分成了5组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f305c8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folders:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_651652/3506847840.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_all_1 = pd.concat([df_all_1, df], ignore_index=True)\n",
      "Processing folders: 100%|██████████| 6/6 [12:57<00:00, 129.62s/it]\n"
     ]
    }
   ],
   "source": [
    "folder_path = '/scratch/work/lyub2/2.df_tc_09'\n",
    "file_prefix = 'df_tc_'\n",
    "file_suffix = '.csv'\n",
    "#total_rows = 0\n",
    "\n",
    "# Initialize an empty DataFrame to store the aggregated missing counts\n",
    "df_all_1 = pd.DataFrame(columns=[\n",
    "    'temp_device', 'tripchain_id', 'leg_order', 'area_id', 'area_order', 'activity', 'km','line_name', 'line_type',\n",
    "    'time_start_distance', 'time_end_distance'])\n",
    "\n",
    "# Loop through each file and process\n",
    "for i in tqdm(range(1, 7), desc=\"Processing folders\"):\n",
    "    file_name = f'{file_prefix}{i:02d}{file_suffix}'\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path, usecols=[\n",
    "        'temp_device', 'tripchain_id', 'leg_order', 'area_id', 'area_order', 'activity', \n",
    "        'km','line_name', 'line_type','time_start_distance', 'time_end_distance'])\n",
    "    \n",
    "    df_all_1 = pd.concat([df_all_1, df], ignore_index=True)\n",
    "\n",
    "df_all_1.to_csv('/scratch/work/lyub2/Problem_output_September/P3/df_all_1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3c3c33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folders:   0%|          | 0/3 [00:00<?, ?it/s]/tmp/ipykernel_819633/2405882279.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_all_2 = pd.concat([df_all_2, df], ignore_index=True)\n",
      "Processing folders: 100%|██████████| 3/3 [06:56<00:00, 138.88s/it]\n"
     ]
    }
   ],
   "source": [
    "folder_path = '/scratch/work/lyub2/2.df_tc_09'\n",
    "file_prefix = 'df_tc_'\n",
    "file_suffix = '.csv'\n",
    "#total_rows = 0\n",
    "\n",
    "# Initialize an empty DataFrame to store the aggregated missing counts\n",
    "df_all_2 = pd.DataFrame(columns=[\n",
    "    'temp_device', 'tripchain_id', 'leg_order', 'area_id', 'area_order', 'activity', 'km','line_name', 'line_type',\n",
    "    'time_start_distance', 'time_end_distance'])\n",
    "\n",
    "# Loop through each file and process\n",
    "for i in tqdm(range(7, 10), desc=\"Processing folders\"):\n",
    "    file_name = f'{file_prefix}{i:02d}{file_suffix}'\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path, usecols=[\n",
    "        'temp_device', 'tripchain_id', 'leg_order', 'area_id', 'area_order', 'activity', \n",
    "        'km','line_name', 'line_type','time_start_distance', 'time_end_distance'])\n",
    "    \n",
    "    df_all_2 = pd.concat([df_all_2, df], ignore_index=True)\n",
    "\n",
    "df_all_2.to_csv('/scratch/work/lyub2/Problem_output_September/P3/df_all_2.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e5d2038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folders:   0%|          | 0/6 [00:00<?, ?it/s]/tmp/ipykernel_929900/45709069.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_all_3 = pd.concat([df_all_3, df], ignore_index=True)\n",
      "Processing folders: 100%|██████████| 6/6 [12:09<00:00, 121.53s/it]\n"
     ]
    }
   ],
   "source": [
    "folder_path = '/scratch/work/lyub2/2.df_tc_09'\n",
    "file_prefix = 'df_tc_'\n",
    "file_suffix = '.csv'\n",
    "#total_rows = 0\n",
    "\n",
    "# Initialize an empty DataFrame to store the aggregated missing counts\n",
    "df_all_3 = pd.DataFrame(columns=[\n",
    "    'temp_device', 'tripchain_id', 'leg_order', 'area_id', 'area_order', 'activity', 'km','line_name', 'line_type',\n",
    "    'time_start_distance', 'time_end_distance'])\n",
    "\n",
    "# Loop through each file and process\n",
    "for i in tqdm(range(10, 16), desc=\"Processing folders\"):\n",
    "    file_name = f'{file_prefix}{i:02d}{file_suffix}'\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path, usecols=[\n",
    "        'temp_device', 'tripchain_id', 'leg_order', 'area_id', 'area_order', 'activity', \n",
    "        'km','line_name', 'line_type','time_start_distance', 'time_end_distance'])\n",
    "    \n",
    "    df_all_3 = pd.concat([df_all_3, df], ignore_index=True)\n",
    "\n",
    "df_all_3.to_csv('/scratch/work/lyub2/Problem_output_September/P3/df_all_3.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "457fd62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folders:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1073657/749393217.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_all_4 = pd.concat([df_all_4, df], ignore_index=True)\n",
      "Processing folders: 100%|██████████| 4/4 [09:13<00:00, 138.28s/it]\n"
     ]
    }
   ],
   "source": [
    "folder_path = '/scratch/work/lyub2/2.df_tc_09'\n",
    "file_prefix = 'df_tc_'\n",
    "file_suffix = '.csv'\n",
    "#total_rows = 0\n",
    "\n",
    "# Initialize an empty DataFrame to store the aggregated missing counts\n",
    "df_all_4 = pd.DataFrame(columns=[\n",
    "    'temp_device', 'tripchain_id', 'leg_order', 'area_id', 'area_order', 'activity', 'km','line_name', 'line_type',\n",
    "    'time_start_distance', 'time_end_distance'])\n",
    "\n",
    "# Loop through each file and process\n",
    "for i in tqdm(range(16, 20), desc=\"Processing folders\"):\n",
    "    file_name = f'{file_prefix}{i:02d}{file_suffix}'\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path, usecols=[\n",
    "        'temp_device', 'tripchain_id', 'leg_order', 'area_id', 'area_order', 'activity', \n",
    "        'km','line_name', 'line_type','time_start_distance', 'time_end_distance'])\n",
    "    \n",
    "    df_all_4 = pd.concat([df_all_4, df], ignore_index=True)\n",
    "\n",
    "df_all_4.to_csv('/scratch/work/lyub2/Problem_output_September/P3/df_all_4.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e05a41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folders:   0%|          | 0/5 [00:00<?, ?it/s]/tmp/ipykernel_1073657/1313498430.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_all_5 = pd.concat([df_all_5, df], ignore_index=True)\n",
      "Processing folders: 100%|██████████| 5/5 [08:47<00:00, 105.55s/it]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "folder_path = '/scratch/work/lyub2/2.df_tc_09'\n",
    "file_prefix = 'df_tc_'\n",
    "file_suffix = '.csv'\n",
    "#total_rows = 0\n",
    "\n",
    "# Initialize an empty DataFrame to store the aggregated missing counts\n",
    "df_all_5 = pd.DataFrame(columns=[\n",
    "    'temp_device', 'tripchain_id', 'leg_order', 'area_id', 'area_order', 'activity', 'km','line_name', 'line_type',\n",
    "    'time_start_distance', 'time_end_distance'])\n",
    "\n",
    "# Loop through each file and process\n",
    "for i in tqdm(range(20, 25), desc=\"Processing folders\"):\n",
    "    file_name = f'{file_prefix}{i:02d}{file_suffix}'\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path, usecols=[\n",
    "        'temp_device', 'tripchain_id', 'leg_order', 'area_id', 'area_order', 'activity', \n",
    "        'km','line_name', 'line_type','time_start_distance', 'time_end_distance'])\n",
    "    \n",
    "    df_all_5 = pd.concat([df_all_5, df], ignore_index=True)\n",
    "\n",
    "df_all_5.to_csv('/scratch/work/lyub2/Problem_output_September/P3/df_all_5.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffc2e46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folders:   0%|          | 0/6 [00:00<?, ?it/s]/tmp/ipykernel_28563/1397153286.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_all_6 = pd.concat([df_all_6, df], ignore_index=True)\n",
      "Processing folders:  83%|████████▎ | 5/6 [08:48<01:42, 102.30s/it]/tmp/ipykernel_28563/1397153286.py:17: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, usecols=[\n",
      "Processing folders: 100%|██████████| 6/6 [09:38<00:00, 96.43s/it] \n"
     ]
    }
   ],
   "source": [
    "folder_path = '/scratch/work/lyub2/2.df_tc_09'\n",
    "file_prefix = 'df_tc_'\n",
    "file_suffix = '.csv'\n",
    "#total_rows = 0\n",
    "\n",
    "# Initialize an empty DataFrame to store the aggregated missing counts\n",
    "df_all_6 = pd.DataFrame(columns=[\n",
    "    'temp_device', 'tripchain_id', 'leg_order', 'area_id', 'area_order', 'activity', 'km','line_name', 'line_type',\n",
    "    'time_start_distance', 'time_end_distance'])\n",
    "\n",
    "# Loop through each file and process\n",
    "for i in tqdm(range(25, 31), desc=\"Processing folders\"):\n",
    "    file_name = f'{file_prefix}{i:02d}{file_suffix}'\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path, usecols=[\n",
    "        'temp_device', 'tripchain_id', 'leg_order', 'area_id', 'area_order', 'activity', \n",
    "        'km','line_name', 'line_type','time_start_distance', 'time_end_distance'])\n",
    "    \n",
    "    df_all_6 = pd.concat([df_all_6, df], ignore_index=True)\n",
    "\n",
    "df_all_6.to_csv('/scratch/work/lyub2/Problem_output_September/P3/df_all_6.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6681a335",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_p1_1, df_p1_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21d6b69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29194643\n"
     ]
    }
   ],
   "source": [
    "# print(len(df_p1_1))\n",
    "# print(len(df_p1_2))\n",
    "print(len(df_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece34189",
   "metadata": {},
   "source": [
    "## 删除含有’wrong detection line name‘的 并进行P3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "926566b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_detection_df= pd.read_csv(\"/scratch/work/lyub2/Problem_output_September/P2/df_wrongline.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d00cc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_1= pd.read_csv(\"/scratch/work/lyub2/Problem_output_September/P1/df_all_1.csv\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68163153-0af0-47dc-b641-4ed99004777b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P3在1/6里, (legs)总长度和 in travelling 的长度 60804922 52908480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4023960/4212058569.py:17: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df[column] = pd.to_datetime(df[column], errors='coerce')\n",
      "/tmp/ipykernel_4023960/4212058569.py:18: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[column] = pd.to_datetime(df[column], errors='coerce', infer_datetime_format=True).dt.tz_convert('Europe/Helsinki')\n",
      "/tmp/ipykernel_4023960/4212058569.py:17: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df[column] = pd.to_datetime(df[column], errors='coerce')\n",
      "/tmp/ipykernel_4023960/4212058569.py:18: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[column] = pd.to_datetime(df[column], errors='coerce', infer_datetime_format=True).dt.tz_convert('Europe/Helsinki')\n",
      "/tmp/ipykernel_4023960/4212058569.py:26: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: round((x['time_end_distance'].max() - x['time_start_distance'].min()).total_seconds() / 60, 2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P3在1/6里，(tripchain)总长度和超出1h的长度： 924823 195326\n"
     ]
    }
   ],
   "source": [
    "##删除有P2的\n",
    "df = df_all_1[~df_all_1['line_name'].isin(wrong_detection_df['line_name'])]\n",
    "\n",
    "#进行P3, 记得得转时间格式\n",
    "activities_to_consider = ['BEACON', 'WALKING', 'RUNNING', 'IN_VEHICLE', 'COMBINED', 'ON_BICYCLE']\n",
    "# Iterate through each dataframe in the dictionary\n",
    "filtered_df = df[['temp_device','activity', 'tripchain_id',  'leg_order', 'time_start_distance', 'time_end_distance']]\n",
    "# Filter the DataFrame\n",
    "filtered_df = filtered_df[filtered_df['activity'].isin(activities_to_consider)]\n",
    "print('P3在1/6里, (legs)总长度和 in travelling 的长度', len(df), len(filtered_df),)\n",
    "      #'Proportion of legs that are in travelling:', len(filtered_df)/len(df_p21))\n",
    "\n",
    "def convert_time_columns_to_Helsinki_datetime(df):\n",
    "    for column in df.columns:\n",
    "        if column.startswith('time'):\n",
    "            # errors='coerce' will handle invalid parsing by setting NaT\n",
    "            df[column] = pd.to_datetime(df[column], errors='coerce')\n",
    "            df[column] = pd.to_datetime(df[column], errors='coerce', infer_datetime_format=True).dt.tz_convert('Europe/Helsinki') \n",
    "    return df\n",
    "convert_time_columns_to_Helsinki_datetime(filtered_df)\n",
    "\n",
    "# Group by tripchain_id and calculate the duration\n",
    "# result = pd.DataFrame(filtered_df.groupby('tripchain_id').apply(\n",
    "# lambda x: round((x['time_end_distance'].max() - x['time_start_distance'].min()).total_seconds()/60, 2)),columns=[\"total_duration_minutes\"]).reset_index()\n",
    "result_df = (filtered_df.groupby(['temp_device', 'tripchain_id'])\n",
    "    .apply(lambda x: round((x['time_end_distance'].max() - x['time_start_distance'].min()).total_seconds() / 60, 2))\n",
    "    .reset_index(name='total_duration_minutes'))\n",
    "\n",
    "more_than_1h = result_df[result_df['total_duration_minutes'] > 60]\n",
    "print('P3在1/6里，(tripchain)总长度和超出1h的长度：', len(result_df), len(more_than_1h))\n",
    "\n",
    "\n",
    "## 然后在P2的基础上删除P3\n",
    "mask = ~df.set_index(['temp_device', 'tripchain_id']).index.isin(\n",
    "    more_than_1h.set_index(['temp_device', 'tripchain_id']).index\n",
    ")\n",
    "\n",
    "# Filter df using the mask\n",
    "df_p41 = df[mask].reset_index(drop=True)\n",
    "df_p41.to_csv('/scratch/work/lyub2/Problem_output_September/P4/df_p41.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37007c1a-ed1b-4e1c-90e5-08f005fdf9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P2在1/6里, (legs)去除P2前和去除P2后的长度 61904380 60804922\n"
     ]
    }
   ],
   "source": [
    "print('P2在1/6里, (legs)去除P2前和去除P2后的长度',len(df_all_1), len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66bfdc44-2cbb-455b-8a41-3bba77c3dade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去除P2P3后的长度 32773312\n"
     ]
    }
   ],
   "source": [
    "print('去除P2P3后的长度', len(df_p41))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "733340c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all_2= pd.read_csv(\"/scratch/work/lyub2/Problem_output_September/P1/df_all_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "122ef22c-e68f-41f3-954b-84ad78766062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P2在2/6里, (legs)去除P2前和去除P2后的长度 32998220 32401192\n",
      "P3在2/6里, (legs)总长度和 in travelling 的长度 32401192 28154701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4035926/2238402059.py:18: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[column] = pd.to_datetime(df[column], errors='coerce', infer_datetime_format=True).dt.tz_convert('Europe/Helsinki')\n",
      "/tmp/ipykernel_4035926/2238402059.py:18: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[column] = pd.to_datetime(df[column], errors='coerce', infer_datetime_format=True).dt.tz_convert('Europe/Helsinki')\n",
      "/tmp/ipykernel_4035926/2238402059.py:26: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: round((x['time_end_distance'].max() - x['time_start_distance'].min()).total_seconds() / 60, 2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P3在2/6里，(tripchain)总长度和超出1h的长度： 483784 106130\n",
      "去除P2P3后的长度 16906025\n"
     ]
    }
   ],
   "source": [
    "##删除有P2的\n",
    "df = df_all_2[~df_all_2['line_name'].isin(wrong_detection_df['line_name'])]\n",
    "print('P2在2/6里, (legs)去除P2前和去除P2后的长度', len(df_all_2), len(df))\n",
    "#进行P3, 记得得转时间格式\n",
    "activities_to_consider = ['BEACON', 'WALKING', 'RUNNING', 'IN_VEHICLE', 'COMBINED', 'ON_BICYCLE']\n",
    "# Iterate through each dataframe in the dictionary\n",
    "filtered_df = df[['temp_device','activity', 'tripchain_id',  'leg_order', 'time_start_distance', 'time_end_distance']]\n",
    "# Filter the DataFrame\n",
    "filtered_df = filtered_df[filtered_df['activity'].isin(activities_to_consider)]\n",
    "print('P3在2/6里, (legs)总长度和 in travelling 的长度', len(df), len(filtered_df),)\n",
    "      #'Proportion of legs that are in travelling:', len(filtered_df)/len(df_p21))\n",
    "\n",
    "def convert_time_columns_to_Helsinki_datetime(df):\n",
    "    for column in df.columns:\n",
    "        if column.startswith('time'):\n",
    "            # errors='coerce' will handle invalid parsing by setting NaT\n",
    "            df[column] = pd.to_datetime(df[column], errors='coerce')\n",
    "            df[column] = pd.to_datetime(df[column], errors='coerce', infer_datetime_format=True).dt.tz_convert('Europe/Helsinki') \n",
    "    return df\n",
    "convert_time_columns_to_Helsinki_datetime(filtered_df)\n",
    "\n",
    "# Group by tripchain_id and calculate the duration\n",
    "# result = pd.DataFrame(filtered_df.groupby('tripchain_id').apply(\n",
    "# lambda x: round((x['time_end_distance'].max() - x['time_start_distance'].min()).total_seconds()/60, 2)),columns=[\"total_duration_minutes\"]).reset_index()\n",
    "result_df = (filtered_df.groupby(['temp_device', 'tripchain_id'])\n",
    "    .apply(lambda x: round((x['time_end_distance'].max() - x['time_start_distance'].min()).total_seconds() / 60, 2))\n",
    "    .reset_index(name='total_duration_minutes'))\n",
    "\n",
    "more_than_1h = result_df[result_df['total_duration_minutes'] > 60]\n",
    "print('P3在2/6里，(tripchain)总长度和超出1h的长度：', len(result_df), len(more_than_1h))\n",
    "\n",
    "\n",
    "## 然后在P2的基础上删除P3\n",
    "mask = ~df.set_index(['temp_device', 'tripchain_id']).index.isin(\n",
    "    more_than_1h.set_index(['temp_device', 'tripchain_id']).index\n",
    ")\n",
    "\n",
    "# Filter df using the mask\n",
    "df_p42 = df[mask].reset_index(drop=True)\n",
    "print('去除P2P3后的长度', len(df_p42))\n",
    "df_p42.to_csv('/scratch/work/lyub2/Problem_output_September/P4/df_p42.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ba44e19-75b7-47c5-9e2a-e1d4d0758cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_3= pd.read_csv(\"/scratch/work/lyub2/Problem_output_September/P1/df_all_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d5199c5-79dd-46a7-b9ca-60e3e87682e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P2在3/6里, (legs)去除P2前和去除P2后的长度 63559191 62341788\n",
      "P3在3/6里, (legs)总长度和 in travelling 的长度 62341788 54416407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4035926/3957622912.py:17: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df[column] = pd.to_datetime(df[column], errors='coerce')\n",
      "/tmp/ipykernel_4035926/3957622912.py:18: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[column] = pd.to_datetime(df[column], errors='coerce', infer_datetime_format=True).dt.tz_convert('Europe/Helsinki')\n",
      "/tmp/ipykernel_4035926/3957622912.py:17: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df[column] = pd.to_datetime(df[column], errors='coerce')\n",
      "/tmp/ipykernel_4035926/3957622912.py:18: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[column] = pd.to_datetime(df[column], errors='coerce', infer_datetime_format=True).dt.tz_convert('Europe/Helsinki')\n",
      "/tmp/ipykernel_4035926/3957622912.py:26: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: round((x['time_end_distance'].max() - x['time_start_distance'].min()).total_seconds() / 60, 2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P3在3/6里，(tripchain)总长度和超出1h的长度： 960644 196186\n",
      "去除P2P3后的长度 34075388\n"
     ]
    }
   ],
   "source": [
    "##删除有P2的\n",
    "df = df_all_3[~df_all_3['line_name'].isin(wrong_detection_df['line_name'])]\n",
    "print('P2在3/6里, (legs)去除P2前和去除P2后的长度', len(df_all_3), len(df))\n",
    "#进行P3, 记得得转时间格式\n",
    "activities_to_consider = ['BEACON', 'WALKING', 'RUNNING', 'IN_VEHICLE', 'COMBINED', 'ON_BICYCLE']\n",
    "# Iterate through each dataframe in the dictionary\n",
    "filtered_df = df[['temp_device','activity', 'tripchain_id',  'leg_order', 'time_start_distance', 'time_end_distance']]\n",
    "# Filter the DataFrame\n",
    "filtered_df = filtered_df[filtered_df['activity'].isin(activities_to_consider)]\n",
    "print('P3在3/6里, (legs)总长度和 in travelling 的长度', len(df), len(filtered_df),)\n",
    "      #'Proportion of legs that are in travelling:', len(filtered_df)/len(df_p21))\n",
    "\n",
    "def convert_time_columns_to_Helsinki_datetime(df):\n",
    "    for column in df.columns:\n",
    "        if column.startswith('time'):\n",
    "            # errors='coerce' will handle invalid parsing by setting NaT\n",
    "            df[column] = pd.to_datetime(df[column], errors='coerce')\n",
    "            df[column] = pd.to_datetime(df[column], errors='coerce', infer_datetime_format=True).dt.tz_convert('Europe/Helsinki') \n",
    "    return df\n",
    "convert_time_columns_to_Helsinki_datetime(filtered_df)\n",
    "\n",
    "# Group by tripchain_id and calculate the duration\n",
    "# result = pd.DataFrame(filtered_df.groupby('tripchain_id').apply(\n",
    "# lambda x: round((x['time_end_distance'].max() - x['time_start_distance'].min()).total_seconds()/60, 2)),columns=[\"total_duration_minutes\"]).reset_index()\n",
    "result_df = (filtered_df.groupby(['temp_device', 'tripchain_id'])\n",
    "    .apply(lambda x: round((x['time_end_distance'].max() - x['time_start_distance'].min()).total_seconds() / 60, 2))\n",
    "    .reset_index(name='total_duration_minutes'))\n",
    "\n",
    "more_than_1h = result_df[result_df['total_duration_minutes'] > 60]\n",
    "print('P3在3/6里，(tripchain)总长度和超出1h的长度：', len(result_df), len(more_than_1h))\n",
    "\n",
    "\n",
    "## 然后在P2的基础上删除P3\n",
    "mask = ~df.set_index(['temp_device', 'tripchain_id']).index.isin(\n",
    "    more_than_1h.set_index(['temp_device', 'tripchain_id']).index\n",
    ")\n",
    "\n",
    "# Filter df using the mask\n",
    "df_p43 = df[mask].reset_index(drop=True)\n",
    "print('去除P2P3后的长度', len(df_p43))\n",
    "df_p43.to_csv('/scratch/work/lyub2/Problem_output_September/P4/df_p43.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e80af9f-502a-468d-be85-b68e879075ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_4= pd.read_csv(\"/scratch/work/lyub2/Problem_output_September/P1/df_all_4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b781a90d-5117-4394-86df-e905097be66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P2在4/6里, (legs)去除P2前和去除P2后的长度 36846955 36188121\n",
      "P3在4/6里, (legs)总长度和 in travelling 的长度 36188121 31492165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4035926/3554944260.py:17: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df[column] = pd.to_datetime(df[column], errors='coerce')\n",
      "/tmp/ipykernel_4035926/3554944260.py:18: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[column] = pd.to_datetime(df[column], errors='coerce', infer_datetime_format=True).dt.tz_convert('Europe/Helsinki')\n",
      "/tmp/ipykernel_4035926/3554944260.py:17: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df[column] = pd.to_datetime(df[column], errors='coerce')\n",
      "/tmp/ipykernel_4035926/3554944260.py:18: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[column] = pd.to_datetime(df[column], errors='coerce', infer_datetime_format=True).dt.tz_convert('Europe/Helsinki')\n",
      "/tmp/ipykernel_4035926/3554944260.py:26: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: round((x['time_end_distance'].max() - x['time_start_distance'].min()).total_seconds() / 60, 2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P3在4/6里，(tripchain)总长度和超出1h的长度： 560655 116028\n",
      "去除P2P3后的长度 19629007\n"
     ]
    }
   ],
   "source": [
    "df = df_all_4[~df_all_4['line_name'].isin(wrong_detection_df['line_name'])]\n",
    "print('P2在4/6里, (legs)去除P2前和去除P2后的长度', len(df_all_4), len(df))\n",
    "\n",
    "#进行P3, 记得得转时间格式\n",
    "activities_to_consider = ['BEACON', 'WALKING', 'RUNNING', 'IN_VEHICLE', 'COMBINED', 'ON_BICYCLE']\n",
    "# Iterate through each dataframe in the dictionary\n",
    "filtered_df = df[['temp_device','activity', 'tripchain_id',  'leg_order', 'time_start_distance', 'time_end_distance']]\n",
    "# Filter the DataFrame\n",
    "filtered_df = filtered_df[filtered_df['activity'].isin(activities_to_consider)]\n",
    "print('P3在4/6里, (legs)总长度和 in travelling 的长度', len(df), len(filtered_df),)\n",
    "      #'Proportion of legs that are in travelling:', len(filtered_df)/len(df_p21))\n",
    "\n",
    "def convert_time_columns_to_Helsinki_datetime(df):\n",
    "    for column in df.columns:\n",
    "        if column.startswith('time'):\n",
    "            # errors='coerce' will handle invalid parsing by setting NaT\n",
    "            df[column] = pd.to_datetime(df[column], errors='coerce')\n",
    "            df[column] = pd.to_datetime(df[column], errors='coerce', infer_datetime_format=True).dt.tz_convert('Europe/Helsinki') \n",
    "    return df\n",
    "convert_time_columns_to_Helsinki_datetime(filtered_df)\n",
    "\n",
    "# Group by tripchain_id and calculate the duration\n",
    "# result = pd.DataFrame(filtered_df.groupby('tripchain_id').apply(\n",
    "# lambda x: round((x['time_end_distance'].max() - x['time_start_distance'].min()).total_seconds()/60, 2)),columns=[\"total_duration_minutes\"]).reset_index()\n",
    "result_df = (filtered_df.groupby(['temp_device', 'tripchain_id'])\n",
    "    .apply(lambda x: round((x['time_end_distance'].max() - x['time_start_distance'].min()).total_seconds() / 60, 2))\n",
    "    .reset_index(name='total_duration_minutes'))\n",
    "\n",
    "more_than_1h = result_df[result_df['total_duration_minutes'] > 60]\n",
    "print('P3在4/6里，(tripchain)总长度和超出1h的长度：', len(result_df), len(more_than_1h))\n",
    "\n",
    "\n",
    "## 然后在P2的基础上删除P3\n",
    "mask = ~df.set_index(['temp_device', 'tripchain_id']).index.isin(\n",
    "    more_than_1h.set_index(['temp_device', 'tripchain_id']).index\n",
    ")\n",
    "\n",
    "# Filter df using the mask\n",
    "df_p44 = df[mask].reset_index(drop=True)\n",
    "print('去除P2P3后的长度', len(df_p44))\n",
    "df_p44.to_csv('/scratch/work/lyub2/Problem_output_September/P4/df_p44.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c436ae6-ddbb-4655-a8a2-dbfde8f1e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_5= pd.read_csv(\"/scratch/work/lyub2/Problem_output_September/P1/df_all_5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63fe7ba7-b35b-486d-8bee-ef2c7382d86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P2在5/6里, (legs)去除P2前和去除P2后的长度 49499313 48532143\n",
      "P3在5/6里, (legs)总长度和 in travelling 的长度 48532143 42236539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1785825/709372690.py:17: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df[column] = pd.to_datetime(df[column], errors='coerce')\n",
      "/tmp/ipykernel_1785825/709372690.py:18: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[column] = pd.to_datetime(df[column], errors='coerce', infer_datetime_format=True).dt.tz_convert('Europe/Helsinki')\n",
      "/tmp/ipykernel_1785825/709372690.py:17: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df[column] = pd.to_datetime(df[column], errors='coerce')\n",
      "/tmp/ipykernel_1785825/709372690.py:18: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[column] = pd.to_datetime(df[column], errors='coerce', infer_datetime_format=True).dt.tz_convert('Europe/Helsinki')\n",
      "/tmp/ipykernel_1785825/709372690.py:26: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: round((x['time_end_distance'].max() - x['time_start_distance'].min()).total_seconds() / 60, 2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P3在5/6里，(tripchain)总长度和超出1h的长度： 743357 155960\n",
      "去除P2P3后的长度 26215546\n"
     ]
    }
   ],
   "source": [
    "df = df_all_5[~df_all_5['line_name'].isin(wrong_detection_df['line_name'])]\n",
    "print('P2在5/6里, (legs)去除P2前和去除P2后的长度', len(df_all_5), len(df))\n",
    "\n",
    "#进行P3, 记得得转时间格式\n",
    "activities_to_consider = ['BEACON', 'WALKING', 'RUNNING', 'IN_VEHICLE', 'COMBINED', 'ON_BICYCLE']\n",
    "# Iterate through each dataframe in the dictionary\n",
    "filtered_df = df[['temp_device','activity', 'tripchain_id',  'leg_order', 'time_start_distance', 'time_end_distance']]\n",
    "# Filter the DataFrame\n",
    "filtered_df = filtered_df[filtered_df['activity'].isin(activities_to_consider)]\n",
    "print('P3在5/6里, (legs)总长度和 in travelling 的长度', len(df), len(filtered_df),)\n",
    "      #'Proportion of legs that are in travelling:', len(filtered_df)/len(df_p21))\n",
    "\n",
    "def convert_time_columns_to_Helsinki_datetime(df):\n",
    "    for column in df.columns:\n",
    "        if column.startswith('time'):\n",
    "            # errors='coerce' will handle invalid parsing by setting NaT\n",
    "            df[column] = pd.to_datetime(df[column], errors='coerce')\n",
    "            df[column] = pd.to_datetime(df[column], errors='coerce', infer_datetime_format=True).dt.tz_convert('Europe/Helsinki') \n",
    "    return df\n",
    "convert_time_columns_to_Helsinki_datetime(filtered_df)\n",
    "\n",
    "# Group by tripchain_id and calculate the duration\n",
    "# result = pd.DataFrame(filtered_df.groupby('tripchain_id').apply(\n",
    "# lambda x: round((x['time_end_distance'].max() - x['time_start_distance'].min()).total_seconds()/60, 2)),columns=[\"total_duration_minutes\"]).reset_index()\n",
    "result_df = (filtered_df.groupby(['temp_device', 'tripchain_id'])\n",
    "    .apply(lambda x: round((x['time_end_distance'].max() - x['time_start_distance'].min()).total_seconds() / 60, 2))\n",
    "    .reset_index(name='total_duration_minutes'))\n",
    "\n",
    "more_than_1h = result_df[result_df['total_duration_minutes'] > 60]\n",
    "print('P3在5/6里，(tripchain)总长度和超出1h的长度：', len(result_df), len(more_than_1h))\n",
    "\n",
    "\n",
    "## 然后在P2的基础上删除P3\n",
    "mask = ~df.set_index(['temp_device', 'tripchain_id']).index.isin(\n",
    "    more_than_1h.set_index(['temp_device', 'tripchain_id']).index\n",
    ")\n",
    "\n",
    "# Filter df using the mask\n",
    "df_p45 = df[mask].reset_index(drop=True)\n",
    "print('去除P2P3后的长度', len(df_p45))\n",
    "df_p45.to_csv('/scratch/work/lyub2/Problem_output_September/P4/df_p45.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6558dfe-119f-4a29-a654-ff351f1cb12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_6= pd.read_csv(\"/scratch/work/lyub2/Problem_output_September/P1/df_all_6.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ba69d0a-a8c1-4633-b926-1b7376b00be8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P2在6/6里, (legs)去除P2前和去除P2后的长度 52821215 51779694\n",
      "P3在6/6里, (legs)总长度和 in travelling 的长度 51779694 45216980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1801860/4031616599.py:17: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df[column] = pd.to_datetime(df[column], errors='coerce')\n",
      "/tmp/ipykernel_1801860/4031616599.py:18: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[column] = pd.to_datetime(df[column], errors='coerce', infer_datetime_format=True).dt.tz_convert('Europe/Helsinki')\n",
      "/tmp/ipykernel_1801860/4031616599.py:17: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df[column] = pd.to_datetime(df[column], errors='coerce')\n",
      "/tmp/ipykernel_1801860/4031616599.py:18: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[column] = pd.to_datetime(df[column], errors='coerce', infer_datetime_format=True).dt.tz_convert('Europe/Helsinki')\n",
      "/tmp/ipykernel_1801860/4031616599.py:26: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: round((x['time_end_distance'].max() - x['time_start_distance'].min()).total_seconds() / 60, 2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P3在6/6里，(tripchain)总长度和超出1h的长度： 831687 166929\n",
      "去除P2P3后的长度 28685720\n"
     ]
    }
   ],
   "source": [
    "df = df_all_6[~df_all_6['line_name'].isin(wrong_detection_df['line_name'])]\n",
    "print('P2在6/6里, (legs)去除P2前和去除P2后的长度', len(df_all_6), len(df))\n",
    "\n",
    "#进行P3, 记得得转时间格式\n",
    "activities_to_consider = ['BEACON', 'WALKING', 'RUNNING', 'IN_VEHICLE', 'COMBINED', 'ON_BICYCLE']\n",
    "# Iterate through each dataframe in the dictionary\n",
    "filtered_df = df[['temp_device','activity', 'tripchain_id',  'leg_order', 'time_start_distance', 'time_end_distance']]\n",
    "# Filter the DataFrame\n",
    "filtered_df = filtered_df[filtered_df['activity'].isin(activities_to_consider)]\n",
    "print('P3在6/6里, (legs)总长度和 in travelling 的长度', len(df), len(filtered_df),)\n",
    "      #'Proportion of legs that are in travelling:', len(filtered_df)/len(df_p21))\n",
    "\n",
    "def convert_time_columns_to_Helsinki_datetime(df):\n",
    "    for column in df.columns:\n",
    "        if column.startswith('time'):\n",
    "            # errors='coerce' will handle invalid parsing by setting NaT\n",
    "            df[column] = pd.to_datetime(df[column], errors='coerce')\n",
    "            df[column] = pd.to_datetime(df[column], errors='coerce', infer_datetime_format=True).dt.tz_convert('Europe/Helsinki') \n",
    "    return df\n",
    "convert_time_columns_to_Helsinki_datetime(filtered_df)\n",
    "\n",
    "# Group by tripchain_id and calculate the duration\n",
    "# result = pd.DataFrame(filtered_df.groupby('tripchain_id').apply(\n",
    "# lambda x: round((x['time_end_distance'].max() - x['time_start_distance'].min()).total_seconds()/60, 2)),columns=[\"total_duration_minutes\"]).reset_index()\n",
    "result_df = (filtered_df.groupby(['temp_device', 'tripchain_id'])\n",
    "    .apply(lambda x: round((x['time_end_distance'].max() - x['time_start_distance'].min()).total_seconds() / 60, 2))\n",
    "    .reset_index(name='total_duration_minutes'))\n",
    "\n",
    "more_than_1h = result_df[result_df['total_duration_minutes'] > 60]\n",
    "print('P3在6/6里，(tripchain)总长度和超出1h的长度：', len(result_df), len(more_than_1h))\n",
    "\n",
    "\n",
    "## 然后在P2的基础上删除P3\n",
    "mask = ~df.set_index(['temp_device', 'tripchain_id']).index.isin(\n",
    "    more_than_1h.set_index(['temp_device', 'tripchain_id']).index\n",
    ")\n",
    "\n",
    "# Filter df using the mask\n",
    "df_p46 = df[mask].reset_index(drop=True)\n",
    "print('去除P2P3后的长度', len(df_p46))\n",
    "df_p46.to_csv('/scratch/work/lyub2/Problem_output_September/P4/df_p46.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9101808-f613-41f8-9309-5a64c03d0526",
   "metadata": {},
   "source": [
    "## 结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec6a1aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297629274 292047860 5581414 \n",
      " 254425272 87.12 \n",
      " 4504950 936559 20.79\n"
     ]
    }
   ],
   "source": [
    "#total legs before filtering out P2P3\n",
    "total_legs = round(61904380  + 32998220 + 63559191 + 36846955 + 49499313 + 52821215, 0)\n",
    "legs_after_p2 = round(60804922 + 32401192 + 62341788 + 36188121 + 48532143 + 51779694, 0)\n",
    "legs_p2 = total_legs - legs_after_p2\n",
    "\n",
    "#legs with travelling activities after filtering P2\n",
    "legs_travelling_after_p2 = round(52908480 + 28154701 + 54416407 + 31492165 + 42236539 + 45216980, 0)\n",
    "proport_travelling_legs = round(legs_travelling_after_p2/legs_after_p2 * 100, 2)\n",
    "\n",
    "#trip chains after filtering out P2\n",
    "tripchains_after_p2 = 924823 + 483784 + 960644 + 560655 + 743357 + 831687\n",
    "tc_more_than_1h = 195326 + 106130 + 196186 + 116028 + 155960 + 166929\n",
    "proport_tc_more_than_1h = round(tc_more_than_1h/tripchains_after_p2 * 100, 2)\n",
    "\n",
    "print(total_legs, legs_after_p2, legs_p2,'\\n' ,\n",
    "      legs_travelling_after_p2, proport_travelling_legs, '\\n',\n",
    "      tripchains_after_p2, tc_more_than_1h, proport_tc_more_than_1h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python generic (scicomp-python-env/2024-01)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
